---
title: "Session 2: Homework 1"
author: "Group 2: Ken Dobson, Fabian Sinn, Carlota Castro Perez, Jieyi Cai, Proud Chaikul, Othman Bensouda"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes 
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(kableExtra)
```

# Where Do People Drink The Most Beer, Wine And Spirits?

Back in 2014, [fivethiryeight.com](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/) published an article on alcohol consumption in different countries.

```{r, load_alcohol_data, echo=FALSE}
library(fivethirtyeight)
data(drinks)

# alcohol_direct <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv")
```

## Checking data quality for drinks dataset 

```{r glimpse_skim_data, echo=FALSE}

#datatypes_drinks <- drinks %>%
#  sapply(drinks,class) %>% 
#  kbl()

anyDuplicated(drinks)

anyNA(drinks)

# glimpse(drinks)
# skim(drinks)
```

Based on checking the dataset, there are no missing values or any duplicates. Datatypes as indicated in the table. 

## Top 25 beer consuming countries

```{r beer_plot}

top_beer <- drinks %>%
  arrange(desc(beer_servings))

ggplot(head(top_beer, 25), aes(x=reorder(country, beer_servings), y = beer_servings)) +
  labs(y = "Servings of Beer", x = "Country", title = "Beer Consumption by Country") +
  geom_bar(stat='identity', show.legend = FALSE, fill = 'lightblue') +
  coord_flip() +
  theme_clean()

```

## Top 25 wine consuming countries

```{r wine_plot}

top_wine <- drinks %>%
  arrange(desc(wine_servings))

ggplot(head(top_wine, 25), aes(x=reorder(country, wine_servings), y = wine_servings)) +
  labs(y = "Servings of Wine", x = "Country", title = "Wine Consumption by Country") +
  geom_bar(stat='identity', show.legend = FALSE, fill = 'lightblue') + 
  coord_flip() +
  theme_clean()


```

## Top 25 spirit consuming countries

```{r spirit_plot}

top_spirits <- drinks %>%
  arrange(desc(spirit_servings))

ggplot(head(top_spirits, 25), aes(x=reorder(country, spirit_servings), y = spirit_servings)) +
  labs(y = "Servings of Spirits", x = "Country", title = "Spirit Consumption by Country") +
  geom_bar(stat='identity', show.legend = FALSE, fill = 'lightblue') + 
  coord_flip() +
  theme_clean()

```

Consumption of alcoholic beverages differs across geographies and exposes national preferences and traditions in terms of beers, spirits and wines. Looking at the example of wine consumption, highest consumption per capita can be seen for countries with traditionally high production of wine (France, Portugal) while Switzerland and Denmark are more on the side of consumption. Overall, consumption patterns in terms of beverages likely align to a large extend with the national cuisine and food preferences, i.e. beer being paired with traditional German style meals. 

# Analysis of movies- IMDB dataset

For the following analysis, we use a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset)

  
```{r,load_movies, warning=FALSE, message=FALSE, echo=FALSE}

movies <- read_csv(here::here("data", "movies.csv"))

```

The dataset consists of the following variable: Besides the obvious variables of `title`, `genre`, `director`, `year`, and `duration`, the rest of the variables are as follows:

* `title`: Movie title
* `genre`: Movie genre
* `director`: Movie director
* `year`: Year the movie was published
* `duration`: Duration of the movie
* `gross`: The gross earnings in the US box office, not adjusted for inflation
* `budget`: The movie's budget 
* `cast_facebook_likes`: the number of facebook likes cast members received
* `votes`: the number of people who voted for (or rated) the movie in IMDB 
* `reviews`: the number of reviews for that movie
* `rating`: IMDB average rating 

## Data exploration of the movies dataset

### Data structure

There are no missing values (NAs) in the dataset. Equally, there are no duplicate rows (= identical for each column) but there is a total of **54** duplicate rows based on titles, director, year and movie duration. The duplicates differ on the number of votes and thus the rating. For the following analysis we removed the second occurrence of each duplicate.

```{r glimpse_skim_data2}

# Checking for missing values
anyNA(movies)

# Checking if there are duplicates based on title, director, year, duration
dupes <- movies %>%
  select(title, director, year, duration) %>%
  duplicated()

table(dupes)["TRUE"]

movies_cleaned <- movies[!dupes,]

```

### Movies per genre

```{r, question2}

movies_by_genre <- movies_cleaned %>%
  group_by(genre) %>%
  count(sort = TRUE)

movies_by_genre %>%
  kbl(col.names = c("Genre", "Movie count")) %>%
  kable_material(c("striped", "hover")) %>%
  kable_styling(fixed_thead = T)

```

### Financial performance

```{r, question3}
genre_gross_budget <- movies_cleaned %>%
  mutate(return_on_budget = gross/budget) %>%
  group_by(genre) %>%
  summarize(mean_gross = mean(gross)/1000000, mean_budget = mean(budget)/1000000, 
            mean_return = mean(return_on_budget)) %>%
  arrange(desc(mean_return))

genre_gross_budget %>% 
 kbl(col.names = c("Genre", "Mean Gross (mm)", "Mean Budget (mm)", "Mean Return"), digits = 2) %>%
  kable_material(c("striped", "hover")) %>%
  kable_styling(fixed_thead = T)
```

This table shows that the most profitable movies are by far horror movies. This genre could be considered as an outlier since it is on average 4 times more profitable than the second on the list, which is Biography. Conversely, the least profitable genre is “Thriller” and it is unexpectedly low. However, it would not be relevant to assert that it is not a profitable genre, since there is only 1 thriller movie in our data frame. As a rule of thumb, we could assume that it would be difficult to conclude without a significant sample size of at least n=30 movies. Hence,  we will not comment on which genre is the least profitable one, but it is fair to conclude that horror movies are indeed very profitable.

### Top directors

```{r, question4}
top_directors <- movies_cleaned %>%
  group_by(director) %>%
  summarize(total = sum(gross)/1000000, mean = mean(gross)/1000000,
            median = median(gross)/1000000, st_dev=STDEV(gross)/1000000) %>%
  arrange(desc(total)) %>%
  head(15)

top_directors %>%
  kbl(col.names = c("Director", "Total Revenue (mm)", "Mean Revenue (mm)", 
                    "Median Revenue (mm)", "SD of Revenue (mm)"), digits = 2) %>%
  kable_material(c("striped", "hover")) %>%
  kable_styling(fixed_thead = T)
```

### Ratings distribution

```{r, question5}
genre_ratings <- movies_cleaned %>%
  group_by(genre) %>%
  summarize(Minimum_Rating = min(rating), Max_Rating = max(rating), 
            Mean_Rating = mean(rating), Median_Rating = median(rating), 
            st_dev=STDEV(rating))
genre_ratings %>%
  kbl(col.names = gsub("[_]", " ", names(genre_ratings))) %>%
  kable_material(c("striped", "hover")) %>%
  kable_styling(fixed_thead = T)

ggplot(movies, aes(x = rating, group = genre)) +
  geom_density(alpha = 0.4, show.legend = F, fill = 'lightblue') +
  labs(title = "Density distribution of ratings by genre", x = "Rating", y = "Density") +
  facet_wrap(~genre) +
  theme_clean()
 
  
```

It is interesting to notice that while horror  movies were the most profitable ones, they are also the most disliked movies (after the thriller movie which represents an unsignificant sample). Conversely, Biography movies are the most liked ones and are also very profitable. Perhaps movies are a great source of inspiration, and while there is no dearth of good fictional movies, what really inspires people are biographies.

## Graphical descriptive analysis based on `ggplot`
  

```{r, gross_on_fblikes}
ggplot(movies, aes(x = cast_facebook_likes, y = gross)) +
  labs(title= "Relationship between facebook likes and \ngross sales of a movie", x = "# Likes", y = "Gross ($)") +
  geom_point() +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(labels = scales::comma) +
  theme_clean()

#Correlation coefficient
movies %>%
  summarize(correlation = cor(cast_facebook_likes,gross)^2)

```

There is no direct relationship between the number of facebook likes and the gross sales of a movie which may likely be due to the fact that movies are not primarily promoted though individual facebook pages. The R squared is 0.0454, indicating no significant correlation between the two variables. 


```{r, gross_on_budget}
ggplot(movies, aes(x = budget, y = gross)) +
  labs(title= "Relationship between budget and gross sales of a movie", x = "Budget ($)", y = "Gross ($)") +
  geom_point() + 
  geom_smooth() +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_continuous(labels = scales::dollar) +
  theme_clean()


#Correlation coefficient
movies %>%
  summarize(correlation = cor(budget,gross)^2)

```

A relationship between the movie budget and the gross sales of a movie can be inferred. There is a clear correlation between a higher budget and a resulting higher likelihood for commercial success in terms of gross sales.

```{r, gross_on_rating}
ggplot(movies, aes(x = rating, y = gross)) +
  labs(title= "Relationship between IMDB rating and gross sales of a movie", x = "IMDB Rating", y = "Gross ($)") +
  geom_point() + 
  geom_smooth() +
  facet_wrap(~genre) +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_continuous() +
  theme_clean()
```

The data indicates that a higher IMDB rating is a somewhat indicator for higher gross sales in the categories of action, adventure, animation, biography and drama. This may likely be due to higher budget requirements in terms of cast, special effects and post production. As the general trajectory that can be observed in relatively flat across the other categories, IMDB rating is only (if at all) a limited factor in explaining variance in gross sales. Furthermore, the dataset might be missing information for movies in the categories musical, romance, thriller and western. Hence, it is not possible to infer any conclusion on the correlation between IMDB Rating and gross sales for those categories.    

# Returns of financial stocks

For the following analysis we use the use the `tidyquant` package to download historical data of stock prices, calculate returns, and examine the distribution of returns. 

```{r load_nyse_data, message=FALSE, warning=FALSE, echo=FALSE}
nyse <- read_csv(here::here("data","nyse.csv"))
```

```{r companies_per_sector}

nyse_comp_sect <- nyse %>% 
  group_by(sector) %>% 
  count(sort = TRUE)

nyse_comp_sect %>%
  kbl(col.names = gsub("[_]", " ", names(nyse_comp_sect))) %>%
  kable_material(c("striped", "hover")) %>%
  kable_styling(fixed_thead = T)

ggplot(nyse_comp_sect, aes(x = n, y = reorder(sector, n))) +
  labs(y = "Sector", x = "# of companies", title = "NYSE - Companies by sector") +
  geom_bar(stat='identity', show.legend = FALSE) + 
  geom_col() +
  theme_clean()

```


In the following section, we chose to download data for the following companies as well as the SP500 ETF:

* `GOOG`: Google
* `RY`: Royal Bank of Canada
* `AZN`: Astrazeneca
* `BP`: British Petroleum
* `DAL`: Delta Air Lines
* `TM`: Toyota
* `AMZN`: Amazon
* `SPY`: SP500 

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE, echo=FALSE}

myStocks <- c("GOOG","RY","AZN","BP","DAL","TM","AMZN","SPY") %>%
  tq_get(get  = "stock.prices",
         from = "2011-01-01",
         to   = "2020-08-31") %>%
  group_by(symbol) 

```

```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE, echo=FALSE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

glimpse(myStocks_returns_monthly)

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

## Monthly returns of stock portfolio and SP500

```{r summarise_monthly_returns}

stock_returns_monthly <- myStocks_returns_monthly%>% 
  group_by(symbol) %>% 
  summarize(Min_return = min(monthly_returns), Max_return = max(monthly_returns), Median_return = median(monthly_returns), Mean_return = mean(monthly_returns), SD_return = sd(monthly_returns))
  
stock_returns_monthly %>%
  kbl(col.names = gsub("[_]", " ", names(stock_returns_monthly))) %>%
  kable_material(c("striped", "hover")) %>%
  kable_styling(fixed_thead = T)

```


```{r density_monthly_returns}

ggplot(myStocks_returns_monthly, aes(x=monthly_returns)) +
  geom_density(fill = "lightblue", alpha = 0.4) +
  labs(title = "Monthly returns of portfolio stocks (2011- 08/2020)", x = "Monthly returns", y = "Density") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) + 
  facet_wrap(~symbol, scales='fixed') +
  theme_clean()

```

Looking at the plots of each stock, the Delta airlines stock is the riskiest as it has the highest volatility. As to be expected, the SP500 shows the lowest volatility compared to the chosen stocks as it tracks the average performance of all stocks in the respective index.

```{r risk_return_plot}

ggplot(stock_returns_monthly, aes(x = SD_return, y = Mean_return, colour = symbol)) +
  geom_point() +
  ggrepel::geom_text_repel(aes(label = symbol)) +
  labs(title = "Expected monthly return of portfolio stocks",
       y = "Expected return (mean)", x = "Risk (standard deviation)") +
  theme_clean() +
  theme(legend.position = 'none')


```

The plot indicates that Amazon is clearly the stock with the strongest risk-reward profile in terms of expected return based on the mean monthly stock performance, followed by Google and Astra Zeneca. Looking at British Petroleum, the stock is underperforming in terms of risk-reward with a high standard deviation and an almost non-existent expected return.  

# IBM HR Analytics

In the following we will analyse will analyse a data set on Human Resoruce Analytics. The [IBM HR Analytics Employee Attrition & Performance data seta](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset) is a fictional data set created by IBM data scientists.  Among other things, the data set includes employees' income, their distance from work, their position in the company, their level of education, etc. A full description can be found on the website.

```{r, echo = FALSE}

hr_dataset <- read_csv(here::here("data", "datasets_1067_1925_WA_Fn-UseC_-HR-Employee-Attrition.csv"))
# glimpse(hr_dataset)

```

```{r, echo=FALSE}

hr_cleaned <- hr_dataset %>% 
  clean_names() %>% 
  mutate(
    education = case_when(
      education == 1 ~ "Below College",
      education == 2 ~ "College",
      education == 3 ~ "Bachelor",
      education == 4 ~ "Master",
      education == 5 ~ "Doctor"
    ),
    environment_satisfaction = case_when(
      environment_satisfaction == 1 ~ "Low",
      environment_satisfaction == 2 ~ "Medium",
      environment_satisfaction == 3 ~ "High",
      environment_satisfaction == 4 ~ "Very High"
    ),
    job_satisfaction = case_when(
      job_satisfaction == 1 ~ "Low",
      job_satisfaction == 2 ~ "Medium",
      job_satisfaction == 3 ~ "High",
      job_satisfaction == 4 ~ "Very High"
    ),
    performance_rating = case_when(
      performance_rating == 1 ~ "Low",
      performance_rating == 2 ~ "Good",
      performance_rating == 3 ~ "Excellent",
      performance_rating == 4 ~ "Outstanding"
    ),
    work_life_balance = case_when(
      work_life_balance == 1 ~ "Bad",
      work_life_balance == 2 ~ "Good",
      work_life_balance == 3 ~ "Better",
      work_life_balance == 4 ~ "Best"
    )
  ) %>% 
  select(age, attrition, daily_rate, department,
         distance_from_home, education,
         gender, job_role,environment_satisfaction,
         job_satisfaction, marital_status,
         monthly_income, num_companies_worked, percent_salary_hike,
         performance_rating, total_working_years,
         work_life_balance, years_at_company,
         years_since_last_promotion)

#glimpse(hr_cleaned)

```


## Checking data quality for HR dataset  

Before starting our analysis, we check data quality. As we can see, there is no missing nor duplicate data.

```{r data_qual_check}

# skim(hr_cleaned)

anyDuplicated(hr_cleaned)

anyNA(hr_cleaned)

```

## Employee attrition 

```{r ibm_attrition}
attrition_rate <- hr_cleaned %>% 
  count(hr_cleaned$attrition) %>% 
  mutate(n/sum(n)*100)

attrition_rate %>%
  kbl(col.names = c("Attrition status", "Count", "Attrition (%)")) %>%
  kable_material(c("striped", "hover")) %>%
  kable_styling(fixed_thead = T)

```

## Distribution of employee data 

```{r summary_stats}
ggplot(hr_cleaned, aes(age)) +
  labs(title = "Boxplot - Age Distribution",  x = "Age") +
  geom_boxplot() +
  theme_classic()

ggplot(hr_cleaned,aes(x=years_at_company)) +
  labs(title = "Boxplot - Company tenure",  x = "Years at company") +
  geom_boxplot() +
  theme_classic()

ggplot(hr_cleaned,aes(x=monthly_income)) + 
  labs(title = "Boxplot - Monthly income",  x = "Income") +
  geom_boxplot() +
  theme_classic()

ggplot(hr_cleaned,aes(x=years_since_last_promotion)) +
  labs(title = "Boxplot - Years since last promotion",  x = "Years since last promotion") +
  geom_boxplot() +
  theme_classic()

```

Using the summary statistics it is clear that age is the closest to normally distributed. The others are highly skewed which is as expected. In a growing company it's necessary to hire staff as responsibilities grow, as such there will be many employees who have not been with the company for a long time. As well, lower level employees typically have a regular promotion schedule so it makes sense that a large portion of workers have been recently promoted. The income distribution is intuitive as many more lower level employees are required relative to the number of executives in order for the company to function.

```{r ibm_summary}

# summary(hr_cleaned)
ggplot(hr_cleaned, aes(x=age)) + 
  geom_histogram(aes(y=..density..)) +
  geom_density(colour = 'red') +
  labs(title = "Distribution of age", x = "Age", y = "Density") +
  theme_clean()

ggplot(hr_cleaned, aes(x=years_at_company)) + 
  geom_histogram(aes(y=..density..)) +
  geom_density(colour = 'red') +
  labs(title = "Distribution of company tenure", x = "Company tenure", y = "Density") +
  theme_clean()

ggplot(hr_cleaned, aes(x=monthly_income)) + 
  geom_histogram(aes(y=..density..)) +
  geom_density(colour = 'red') +
  labs(title = "Distribution of monthly income", x = "Monthly income", y = "Density") +
  theme_clean()

ggplot(hr_cleaned, aes(x=years_since_last_promotion)) + 
  geom_histogram(aes(y=..density..)) +
  geom_density(colour = 'red') +
  labs(title = "Distribution of years since last promotion", x = "Years since last promotion", y = "Density") +
  theme_clean()

```

Age is the factor closest mimicking a normal distribution as can be seen by the relatively similar median and mean values, as well as the bell shape. Years since last promotion etc. are not likely to follow a normal distribution they are skewed to the right.

## Distribution of satisfaction indicators

```{r wlb}
level_order <- c('Low', 'Medium', 'High', 'Very High')

ggplot(hr_cleaned, aes(x = factor(job_satisfaction, level = level_order))) +
  geom_bar(aes(y=(..count..)/sum(..count..)), binwidth = 0.5) +
  labs(title = "Distribution of employee job satisfaction", x = "Job satisfaction", y = "Percentage") +
  scale_y_continuous(labels = scales::percent) +
  theme_clean()
```


```{r}
level_order <- c('Bad', 'Good', 'Better', 'Best')

ggplot(hr_cleaned, aes(x = factor(work_life_balance, level = level_order))) +
  geom_bar(aes(y=(..count..)/sum(..count..)), binwidth = 0.5) +
  labs(title = "Distribution of employee work-life-balance", x = "Work-life-balance", y = "Percentage") +
  scale_y_continuous(labels = scales::percent) +
  theme_clean()
```

## Relationship between income and education/ gender

```{r income_ed}
level_order <- c('Below College', 'College', 'Bachelor', 'Master', 'Doctor')

ggplot(hr_cleaned, aes(x = factor(education, level = level_order), y = monthly_income)) + 
  geom_boxplot() +
  labs(title = "Relationship of education and income", x = "Education", y = "Monthly income") +
  theme_clean()
```


Looking at the relationship between educational level and expected monthly income, there is a clear indication that a higher degree can lead to higher monetary outcomes. Though, this is only one of the factors influencing the pay and profession, title, career progression (among others) likely have a significant influence. Thus, we see high variability in pay. 

```{r income_gend}
ggplot(hr_cleaned, aes(x = gender, y = monthly_income)) + 
  geom_boxplot() +
  labs(title = "Relationship of gender and income", x = "Gender", y = "Monthly income ($)") +
  theme_clean()
```

The data in regarding the relationship between gender and pay indicates that for the given dataset female employees are on average paid a higher monthly salary.  

## Income distribution by job role

```{r income_job}
ggplot(hr_cleaned, aes(x = reorder(job_role, monthly_income), y = monthly_income)) + 
  geom_boxplot() +
  labs(title = "Monthly income distribution by job title", x = "Job title", y = "Monthly income ($)") +
  coord_flip() +
  theme_clean()
```

The monthly income distribution indicates clear disparities among job titles. Managers and research directors are on average paid three times as much as any other job title, which could eventually represent internal inequity and encourage employee attrition.

## Distribution of income by educational level

```{r mean_income_ed}
edu_income <- hr_cleaned %>%
  group_by(education) %>%
  summarize(median = median(monthly_income))
  ggplot(edu_income, aes(x = reorder(education, median), y = median)) +
  geom_bar(stat = 'identity', show.legend = F, fill = 'lightblue') +
  labs(title = "Median income by educational level", x = "Educational level", y = "Monthly income ($)") +
  theme_clean()
 
```

As expected, income is proportional to the educational level.

```{r}
  ggplot(hr_cleaned, aes(x = monthly_income)) +
  geom_density(fill = 'lightblue', show.legend = FALSE, alpha = 0.4) +
  facet_wrap(~education) +
  labs(title = "Income distribution by educational level", x = "Income ($)", y = "Density") +
  theme_clean()
```

The highest salaries are given to a few people, and this is the case for each educational level. This is not surprising as income distribution is almost always skewed to the right in real life.

## Distribution of income by age by job role

```{r}
ggplot(hr_cleaned, aes(x = age, y = monthly_income)) +
  geom_point(show.legend = F) +
  geom_smooth() +
  facet_wrap(~job_role) +
  labs(title = "") +
  labs(title = "Monthly income by job role by age", x = "Age", y = "Job role") +
  theme_clean()
```

The data about different job roles indicates that average pay increase by age differs, e.g. managers receive a higher expected increase in compensation than laboratory technicians who face a relatively flat income development. 

# Challenge 1: Replicating a chart

In the following we are replicating a publication-ready plot. We use the journal article "Riddell_Annals_Hom-Sui-Disparities.pdf" Figure 3 and the "CDC_Males.csv" file.

```{r challenge1, echo=FALSE, out.width="90%"}
knitr::include_graphics(here::here("images", "figure3.jpeg"), error = FALSE)
```

```{r, echo=FALSE}
# - https://cran.r-project.org/web/packages/ggrepel/vignettes/ggrepel.html
# - http://colorbrewer2.org
```


```{r, echo=FALSE}
# Replicate Figure 3
male_pop <- vroom::vroom(here::here("data","CDC_Males.csv"))
```

```{r,fig.width=8,fig.height=6}
#Filter out the duplicate/NA information for gun ownership
gun_ownership <- male_pop %>%
  filter(!is.na(gun.house.prev.category)) %>%
  select(ST, average.pop.white, gun.house.prev.category) %>%
  distinct()

#Get the necessary murder/homicide data for the graph
white_males <- male_pop %>%
  filter(type.fac == "Firearm-related") %>%
  select(ST, adjusted.homicide.White, adjusted.suicide.White)

#Join the two tables 
complete_data <- gun_ownership %>%
  inner_join(white_males, by = "ST")

ggplot(complete_data, aes(x = adjusted.suicide.White, y = adjusted.homicide.White, label = ST)) +
  geom_point(aes(fill = factor(gun.house.prev.category),
                 size = average.pop.white), pch = 21) +
  scale_fill_manual(values = c("#fdf0d9", "#fdcc8a", "#fb8d58", "#d6301e"), name = "Gun ownership") +
  scale_size_continuous(name = "White population", breaks = c(500000, 1500000, 3000000, 7000000),
                      labels = c("500k", "1.5m", "3m", "7m"), range = c(1,15),
                      limits = c(0, 8000000)) +
  labs(x = "White Homicide Rate (per 100,000 per year)", y = "White Suicide Rate (per 100,000 per year)") +
  ggrepel::geom_text_repel() +
  annotate(geom="text", x=25, y=0.75, size =3, label="Spearman's rho = 0.74") +
  theme(panel.grid = element_line(colour = "#f0f0f0"),
        panel.background = element_rect(colour = "black", size=0.5, fill = NA),
        legend.key = element_rect(colour = "transparent", fill = "transparent")) +
        guides(fill = guide_legend(order = 1, override.aes = list(size = 5)),
         size = guide_legend(order = 2))


```

# Challenge 2: 2016 California Contributors plots

In the following we reproduce  the plot that shows the top ten cities in highest amounts raised in political contributions in California during the 2016 US Presidential election.

```{r challenge2, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "challenge2.png"), error = FALSE)
```

```{r, load_CA_data, warnings= FALSE, message=FALSE, echo=FALSE}
# Using vroom() as it is significantly faster than read.csv()
CA_contributors_2016 <- vroom::vroom(here::here("data","CA_contributors_2016.csv"))
zip_codes <- vroom::vroom(here::here("data","zip_code_database.csv"))

```

```{r,fig.width=12,fig.height=5}
library(patchwork)
library(tidytext)
#Get total donations per zip code per candidate
zip_donations <- CA_contributors_2016 %>%
  group_by(cand_nm, zip) %>%
  summarize(total_by_zip = sum(contb_receipt_amt)) %>%
  ungroup()
#Filter out the unnecessary information in the zip_codes data
cali_cities <- zip_codes %>%
  filter(state == "CA") %>%
  transform(zip = as.double(zip)) %>%
  select(zip, primary_city)

#Find the donations per city for all candidates
donations_by_city <- zip_donations %>%
  inner_join(cali_cities, by = "zip") %>%
  group_by(cand_nm, primary_city) %>%
  summarize(total_by_city = sum(total_by_zip)) %>%
  ungroup()

#Narrow it down to just the two presidential nominees and their top 10 cities
pres_noms <- donations_by_city %>%
  filter(cand_nm == "Trump, Donald J." | cand_nm == "Clinton, Hillary Rodham") %>%
  group_by(cand_nm) %>%
  slice_max(n=10, order_by = total_by_city) %>%
  ungroup()  # mutate(cand_nm = as.factor(cand_nm), reorder_within(primary_city, total_by_city, cand_nm))
# pres_noms


p1 <- ggplot(pres_noms, aes(x=reorder_within(primary_city, total_by_city, cand_nm), total_by_city, y = total_by_city)) +
  geom_bar(stat='identity', aes(color = as.factor(cand_nm), fill = as.factor(cand_nm)), show.legend = FALSE) +
  coord_flip() +
  labs(title = "Where did candidates raise the most money?", y = "Amount raised") +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_reordered() +
  scale_fill_manual(values = c("#2e74c0", "#cb454a")) +
  scale_color_manual(values = c("#bcd3eb", "#eab2b4")) +
  ggtitle("Where did candidates raise the most money?") +
  facet_wrap(~cand_nm, scales = "free") +
  theme(strip.background = element_rect(fill="lightGrey", color = "black", size = 0.5),
              strip.text = element_text(size=8),
              panel.border = element_rect(colour = "black", size=0.5, fill = NA, linetype = "solid"),
              panel.background = element_rect(fill = "white"),
              panel.grid = element_line(colour = "#f0f0f0"),
              plot.title = element_text(size = 8),
              axis.title.y = element_blank()) + theme(plot.title = element_text(size = 14))

p1

```

# Details

* Who did you collaborate with: Ken Dobson, Fabian Sinn, Carlota Castro Perez, Jieyi Cai, Proud Chaikul, Othman Bensouda
* Approximately how much time did you spend on this problem set: 48 hours
*  What, if anything, gave you the most trouble: Graphics for Challenge 1




